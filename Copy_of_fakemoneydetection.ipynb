{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of fakemoneydetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyorindeTayo/Fake-money-detection-/blob/master/Copy_of_fakemoneydetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPqRQQHbJmKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5L9QULpKnfR",
        "colab_type": "code",
        "outputId": "c3a9a4d9-a80f-47cf-89f8-5ac1fa0e2050",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# Loading the data with the pandas read_csv attribute\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb32713a-d9db-4c29-a841-0c5d14cfe725\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-eb32713a-d9db-4c29-a841-0c5d14cfe725\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_money.csv to data_money (2).csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_money.csv': b'4.0127,10.1477,-3.9366,-4.0728,0\\r\\n2.6606,3.1681,1.9619,0.18662,0\\r\\n3.931,1.8541,-0.02343,1.2314,0\\r\\n0.01727,8.693,1.3989,-3.9668,0\\r\\n3.2414,0.40971,1.4015,1.1952,0\\r\\n2.2504,3.5757,0.35273,0.2836,0\\r\\n-1.3971,3.3191,-1.3927,-1.9948,1\\r\\n0.39012,-0.14279,-0.03199,0.35084,1\\r\\n-1.6677,-7.1535,7.8929,0.96765,1\\r\\n-3.8483,-12.8047,15.6824,-1.281,1\\r\\n-3.5681,-8.213,10.083,0.96765,1\\r\\n-2.2804,-0.30626,1.3347,1.3763,1\\r\\n-1.7582,2.7397,-2.5323,-2.234,1\\r\\n-0.89409,3.1991,-1.8219,-2.9452,1\\r\\n0.3434,0.12415,-0.28733,0.14654,1\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed0WTgl1LETB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the dataset\n",
        "def read_dataset():\n",
        "    df=pd.read_csv('data_money.csv')\n",
        "    data = pd.DataFrame(df)\n",
        "    \n",
        "    # print(len(df.columns))\n",
        "    X = df[df.columns[0:4]].values\n",
        "    y = df[df.columns[4]]\n",
        " \n",
        "    # Encode the dependent variable\n",
        "    Y = one_hot_encode(y)\n",
        "    print(X.shape)\n",
        "    return (X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVd6ttPNLnVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(labels):\n",
        "    n_labels = len(labels)\n",
        "    n_unique_labels = len(np.unique(labels))\n",
        "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
        "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
        "    return one_hot_encode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtH1lI8uL2mw",
        "colab_type": "code",
        "outputId": "e4ab668d-5c75-4c08-eea3-295462a47700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read the dataset\n",
        "X, Y = read_dataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT1I3HKoMCcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle the dataset to mix up the rows.\n",
        "X, Y = shuffle(X, Y, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR11cFRJMXkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the dataset into train and test part\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.20, random_state=415)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-UU6QsIQMle",
        "colab_type": "code",
        "outputId": "bbb61abe-fb24-4157-d507-85c229ce26ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Inpect the shape of the training and testing.\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11, 4)\n",
            "(11, 2)\n",
            "(3, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMsExbuaQUWg",
        "colab_type": "code",
        "outputId": "19fe4966-002e-494c-de0e-0a5dc3b89142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define the important parameters and variable to work with the tensors\n",
        "learning_rate = 0.3\n",
        "training_epochs = 100\n",
        "cost_history = np.empty(shape=[1], dtype=float)\n",
        "n_dim = X.shape[1]\n",
        "print(\"n_dim\", n_dim)\n",
        "n_class = 2\n",
        "model_path =pd.read_csv('data_money.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_dim 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wMlDXoDrosi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the number of hidden layers and number of neurons for each layer\n",
        "n_hidden_1 = 4\n",
        "n_hidden_2 = 4\n",
        "n_hidden_3 = 4\n",
        "n_hidden_4 = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSmlxc9J3KjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, n_dim])\n",
        "W = tf.Variable(tf.zeros([n_dim, n_class]))\n",
        "b = tf.Variable(tf.zeros([n_class]))\n",
        "y_ = tf.placeholder(tf.float32, [None, n_class])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9U2DwGx3lgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the weights and the biases for each layer\n",
        " \n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
        "    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n",
        "    'out': tf.Variable(tf.truncated_normal([n_hidden_4, n_class]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
        "    'b4': tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
        "    'out': tf.Variable(tf.truncated_normal([n_class]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si-4og1BRHLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the model\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "    # Hidden layer with RELU activationsd\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    # Hidden layer with sigmoid activation\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # Hidden layer with sigmoid activation\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "    # Hidden layer with RELU activation\n",
        "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
        "    layer_4 = tf.nn.sigmoid(layer_4)\n",
        "    # Output layer with linear activation\n",
        "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiyvA-JbTGYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize all the variables\n",
        " \n",
        "init = tf.global_variables_initializer()\n",
        " \n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzKiyQzlTLi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call your model defined\n",
        "y = multilayer_perceptron(x, weights, biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg0yuL_rTLZW",
        "colab_type": "code",
        "outputId": "2fb8efa8-8d4f-4186-b8ec-1e31edb33bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "# Define the cost function and optimizer\n",
        "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
        "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
        " \n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-71-db379f83715c>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAW9vKSWTUAJ",
        "colab_type": "code",
        "outputId": "9757549a-48e0-43cf-ef42-86d2397e59b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Calculate the cost and the accuracy for each epoch\n",
        " \n",
        "mse_history = []\n",
        "accuracy_history = []\n",
        " \n",
        "for epoch in range(training_epochs):\n",
        "    sess.run(training_step, feed_dict={x: train_x, y_: train_y})\n",
        "    cost = sess.run(cost_function, feed_dict={x: train_x, y_: train_y})\n",
        "    cost_history = np.append(cost_history, cost)\n",
        "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    # print(\"Accuracy: \", (sess.run(accuracy, feed_dict={x: test_x, y_: test_y})))\n",
        "    pred_y = sess.run(y, feed_dict={x: test_x})\n",
        "    mse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "    mse_ = sess.run(mse)\n",
        "    mse_history.append(mse_)\n",
        "    accuracy = (sess.run(accuracy, feed_dict={x: train_x, y_: train_y}))\n",
        "    accuracy_history.append(accuracy)\n",
        " \n",
        "    print('epoch : ', epoch, ' - ', 'cost: ', cost, \" - MSE: \", mse_, \"- Train Accuracy: \", accuracy)\n",
        "from os.path import join as pjoin\n",
        "save_path = saver.save(sess, model_path)\n",
        "print(\"Model saved in file: %s\" % save_path)\n",
        " \n",
        "#Plot Accuracy Graph\n",
        "plt.plot(accuracy_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        " \n",
        "# Print the final accuracy\n",
        " \n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(\"Test Accuracy: \", (sess.run(accuracy, feed_dict={x: test_x, y_: test_y})))\n",
        " \n",
        "# Print the final mean square error\n",
        " \n",
        "pred_y = sess.run(y, feed_dict={x: test_x})\n",
        "mse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "print(\"MSE: %.4f\" % sess.run(mse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch :  0  -  cost:  0.004496428  - MSE:  3.4401465541678653 - Train Accuracy:  1.0\n",
            "epoch :  1  -  cost:  0.004487668  - MSE:  3.442639745104513 - Train Accuracy:  1.0\n",
            "epoch :  2  -  cost:  0.0044788984  - MSE:  3.445584096228299 - Train Accuracy:  1.0\n",
            "epoch :  3  -  cost:  0.004470258  - MSE:  3.4480708133871745 - Train Accuracy:  1.0\n",
            "epoch :  4  -  cost:  0.0044615734  - MSE:  3.4510058485107677 - Train Accuracy:  1.0\n",
            "epoch :  5  -  cost:  0.004452965  - MSE:  3.4534855715051425 - Train Accuracy:  1.0\n",
            "epoch :  6  -  cost:  0.004444324  - MSE:  3.4564116712661117 - Train Accuracy:  1.0\n",
            "epoch :  7  -  cost:  0.0044357795  - MSE:  3.4593309467148496 - Train Accuracy:  1.0\n",
            "epoch :  8  -  cost:  0.004427225  - MSE:  3.461799888345279 - Train Accuracy:  1.0\n",
            "epoch :  9  -  cost:  0.004418745  - MSE:  3.464709892730897 - Train Accuracy:  1.0\n",
            "epoch :  10  -  cost:  0.0044102655  - MSE:  3.467172161411732 - Train Accuracy:  1.0\n",
            "epoch :  11  -  cost:  0.004401851  - MSE:  3.470073474899786 - Train Accuracy:  1.0\n",
            "epoch :  12  -  cost:  0.004393436  - MSE:  3.4725291160540315 - Train Accuracy:  1.0\n",
            "epoch :  13  -  cost:  0.004385064  - MSE:  3.475421165400976 - Train Accuracy:  1.0\n",
            "epoch :  14  -  cost:  0.0043767462  - MSE:  3.4778702491378337 - Train Accuracy:  1.0\n",
            "epoch :  15  -  cost:  0.004368439  - MSE:  3.480753111292574 - Train Accuracy:  1.0\n",
            "epoch :  16  -  cost:  0.0043601426  - MSE:  3.4831959288140726 - Train Accuracy:  1.0\n",
            "epoch :  17  -  cost:  0.0043519107  - MSE:  3.4860702253010936 - Train Accuracy:  1.0\n",
            "epoch :  18  -  cost:  0.0043437215  - MSE:  3.4885059562732725 - Train Accuracy:  1.0\n",
            "epoch :  19  -  cost:  0.004335543  - MSE:  3.4913711175655746 - Train Accuracy:  1.0\n",
            "epoch :  20  -  cost:  0.004327365  - MSE:  3.4938014997808167 - Train Accuracy:  1.0\n",
            "epoch :  21  -  cost:  0.0043192734  - MSE:  3.4966576749808453 - Train Accuracy:  1.0\n",
            "epoch :  22  -  cost:  0.004311181  - MSE:  3.499081008302434 - Train Accuracy:  1.0\n",
            "epoch :  23  -  cost:  0.0043031108  - MSE:  3.5019292471818706 - Train Accuracy:  1.0\n",
            "epoch :  24  -  cost:  0.0042950837  - MSE:  3.504345651246602 - Train Accuracy:  1.0\n",
            "epoch :  25  -  cost:  0.004287088  - MSE:  3.5071847459695142 - Train Accuracy:  1.0\n",
            "epoch :  26  -  cost:  0.004279136  - MSE:  3.509595187552583 - Train Accuracy:  1.0\n",
            "epoch :  27  -  cost:  0.004271184  - MSE:  3.5124264470952666 - Train Accuracy:  1.0\n",
            "epoch :  28  -  cost:  0.0042632967  - MSE:  3.514830765401944 - Train Accuracy:  1.0\n",
            "epoch :  29  -  cost:  0.0042553875  - MSE:  3.517652377964474 - Train Accuracy:  1.0\n",
            "epoch :  30  -  cost:  0.0042475536  - MSE:  3.520050855086517 - Train Accuracy:  1.0\n",
            "epoch :  31  -  cost:  0.004239731  - MSE:  3.522864526134004 - Train Accuracy:  1.0\n",
            "epoch :  32  -  cost:  0.00423193  - MSE:  3.5252565020368647 - Train Accuracy:  1.0\n",
            "epoch :  33  -  cost:  0.004224172  - MSE:  3.528061551520704 - Train Accuracy:  1.0\n",
            "epoch :  34  -  cost:  0.004216467  - MSE:  3.5304467942033484 - Train Accuracy:  1.0\n",
            "epoch :  35  -  cost:  0.0042087417  - MSE:  3.533243973831715 - Train Accuracy:  1.0\n",
            "epoch :  36  -  cost:  0.0042010476  - MSE:  3.535623290057412 - Train Accuracy:  1.0\n",
            "epoch :  37  -  cost:  0.004193418  - MSE:  3.5384117017305443 - Train Accuracy:  1.0\n",
            "epoch :  38  -  cost:  0.0041858004  - MSE:  3.540785141802207 - Train Accuracy:  1.0\n",
            "epoch :  39  -  cost:  0.0041781818  - MSE:  3.5435655264588806 - Train Accuracy:  1.0\n",
            "epoch :  40  -  cost:  0.0041706394  - MSE:  3.546340529686874 - Train Accuracy:  1.0\n",
            "epoch :  41  -  cost:  0.004163064  - MSE:  3.548703578174521 - Train Accuracy:  1.0\n",
            "epoch :  42  -  cost:  0.0041555855  - MSE:  3.5514696102158023 - Train Accuracy:  1.0\n",
            "epoch :  43  -  cost:  0.0041480856  - MSE:  3.5538275412179936 - Train Accuracy:  1.0\n",
            "epoch :  44  -  cost:  0.0041406616  - MSE:  3.556585724344238 - Train Accuracy:  1.0\n",
            "epoch :  45  -  cost:  0.0041332156  - MSE:  3.5589372275416387 - Train Accuracy:  1.0\n",
            "epoch :  46  -  cost:  0.0041258233  - MSE:  3.5616875519114264 - Train Accuracy:  1.0\n",
            "epoch :  47  -  cost:  0.004118431  - MSE:  3.5640332928448744 - Train Accuracy:  1.0\n",
            "epoch :  48  -  cost:  0.004111071  - MSE:  3.5667764092686487 - Train Accuracy:  1.0\n",
            "epoch :  49  -  cost:  0.004103744  - MSE:  3.5691160601933727 - Train Accuracy:  1.0\n",
            "epoch :  50  -  cost:  0.004096481  - MSE:  3.5718508298752254 - Train Accuracy:  1.0\n",
            "epoch :  51  -  cost:  0.0040891534  - MSE:  3.5741847321375317 - Train Accuracy:  1.0\n",
            "epoch :  52  -  cost:  0.004081944  - MSE:  3.5769113389942273 - Train Accuracy:  1.0\n",
            "epoch :  53  -  cost:  0.0040747137  - MSE:  3.57923946166281 - Train Accuracy:  1.0\n",
            "epoch :  54  -  cost:  0.0040675155  - MSE:  3.581958127940806 - Train Accuracy:  1.0\n",
            "epoch :  55  -  cost:  0.00406035  - MSE:  3.5842803498637323 - Train Accuracy:  1.0\n",
            "epoch :  56  -  cost:  0.0040531834  - MSE:  3.5869910683834227 - Train Accuracy:  1.0\n",
            "epoch :  57  -  cost:  0.004046071  - MSE:  3.589307084833759 - Train Accuracy:  1.0\n",
            "epoch :  58  -  cost:  0.0040389914  - MSE:  3.592009673757016 - Train Accuracy:  1.0\n",
            "epoch :  59  -  cost:  0.0040319227  - MSE:  3.594320789022067 - Train Accuracy:  1.0\n",
            "epoch :  60  -  cost:  0.004024875  - MSE:  3.59701656988625 - Train Accuracy:  1.0\n",
            "epoch :  61  -  cost:  0.004017859  - MSE:  3.599321662870461 - Train Accuracy:  1.0\n",
            "epoch :  62  -  cost:  0.004010866  - MSE:  3.602009158772171 - Train Accuracy:  1.0\n",
            "epoch :  63  -  cost:  0.004003894  - MSE:  3.6043084894311654 - Train Accuracy:  1.0\n",
            "epoch :  64  -  cost:  0.003996943  - MSE:  3.606987900294529 - Train Accuracy:  1.0\n",
            "epoch :  65  -  cost:  0.003990025  - MSE:  3.6092816918358 - Train Accuracy:  1.0\n",
            "epoch :  66  -  cost:  0.003983107  - MSE:  3.6119542303221017 - Train Accuracy:  1.0\n",
            "epoch :  67  -  cost:  0.003976253  - MSE:  3.61424251168719 - Train Accuracy:  1.0\n",
            "epoch :  68  -  cost:  0.0039693885  - MSE:  3.616907644173638 - Train Accuracy:  1.0\n",
            "epoch :  69  -  cost:  0.0039625894  - MSE:  3.619189753722077 - Train Accuracy:  1.0\n",
            "epoch :  70  -  cost:  0.003955746  - MSE:  3.6218477108419616 - Train Accuracy:  1.0\n",
            "epoch :  71  -  cost:  0.00394899  - MSE:  3.624499856375324 - Train Accuracy:  1.0\n",
            "epoch :  72  -  cost:  0.003942222  - MSE:  3.6267732032971343 - Train Accuracy:  1.0\n",
            "epoch :  73  -  cost:  0.0039355303  - MSE:  3.629418235853683 - Train Accuracy:  1.0\n",
            "epoch :  74  -  cost:  0.0039287726  - MSE:  3.631686056756335 - Train Accuracy:  1.0\n",
            "epoch :  75  -  cost:  0.003922113  - MSE:  3.6343243567212933 - Train Accuracy:  1.0\n",
            "epoch :  76  -  cost:  0.0039154543  - MSE:  3.636585570688822 - Train Accuracy:  1.0\n",
            "epoch :  77  -  cost:  0.003908794  - MSE:  3.6392163620582445 - Train Accuracy:  1.0\n",
            "epoch :  78  -  cost:  0.003902188  - MSE:  3.6414735872761113 - Train Accuracy:  1.0\n",
            "epoch :  79  -  cost:  0.0038955926  - MSE:  3.6440974134839696 - Train Accuracy:  1.0\n",
            "epoch :  80  -  cost:  0.0038889868  - MSE:  3.6463481771656387 - Train Accuracy:  1.0\n",
            "epoch :  81  -  cost:  0.0038824675  - MSE:  3.6489647574588946 - Train Accuracy:  1.0\n",
            "epoch :  82  -  cost:  0.0038759152  - MSE:  3.6512101494274187 - Train Accuracy:  1.0\n",
            "epoch :  83  -  cost:  0.0038694062  - MSE:  3.6538197001006556 - Train Accuracy:  1.0\n",
            "epoch :  84  -  cost:  0.0038629295  - MSE:  3.6560600467770654 - Train Accuracy:  1.0\n",
            "epoch :  85  -  cost:  0.003856453  - MSE:  3.6586616177741598 - Train Accuracy:  1.0\n",
            "epoch :  86  -  cost:  0.00385003  - MSE:  3.6608968654186373 - Train Accuracy:  1.0\n",
            "epoch :  87  -  cost:  0.003843618  - MSE:  3.663492336989257 - Train Accuracy:  1.0\n",
            "epoch :  88  -  cost:  0.0038371847  - MSE:  3.665722289722586 - Train Accuracy:  1.0\n",
            "epoch :  89  -  cost:  0.0038308264  - MSE:  3.6683097381936562 - Train Accuracy:  1.0\n",
            "epoch :  90  -  cost:  0.0038244792  - MSE:  3.6705346443314544 - Train Accuracy:  1.0\n",
            "epoch :  91  -  cost:  0.003818121  - MSE:  3.6731157993549766 - Train Accuracy:  1.0\n",
            "epoch :  92  -  cost:  0.0038118274  - MSE:  3.675334405993605 - Train Accuracy:  1.0\n",
            "epoch :  93  -  cost:  0.0038055235  - MSE:  3.6779087763071256 - Train Accuracy:  1.0\n",
            "epoch :  94  -  cost:  0.0037992625  - MSE:  3.680123180723193 - Train Accuracy:  1.0\n",
            "epoch :  95  -  cost:  0.0037929902  - MSE:  3.682689670606658 - Train Accuracy:  1.0\n",
            "epoch :  96  -  cost:  0.0037867613  - MSE:  3.6848984132306772 - Train Accuracy:  1.0\n",
            "epoch :  97  -  cost:  0.0037805436  - MSE:  3.687459500642824 - Train Accuracy:  1.0\n",
            "epoch :  98  -  cost:  0.0037743577  - MSE:  3.689663074333197 - Train Accuracy:  1.0\n",
            "epoch :  99  -  cost:  0.003768172  - MSE:  3.692217032559005 - Train Accuracy:  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-9d08208aed12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' - '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cost: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" - MSE: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"- Train Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoin\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved in file: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m       \u001b[0mcheckpoint_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlatest_filename\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m         \u001b[0;31m# Guard against collision between data file and checkpoint state file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mbasename\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m\"\"\"Returns the final component of a pathname\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not DataFrame"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmNxxqHcTT3S",
        "colab_type": "code",
        "outputId": "3b05db96-1845-4f9b-a63e-0899ad1cdda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "#Plot Accuracy Graph\n",
        "plt.plot(accuracy_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1GRASJKaDAhJUpMBIUlqMiAkSU3/H2X6Lzw2U7CvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fbba-5FTZIf",
        "colab_type": "code",
        "outputId": "2586a1ba-b69e-448b-a85d-1687cdc08833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Print the final accuracy\n",
        " \n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print(\"Test Accuracy: \", (sess.run(accuracy, feed_dict={x: test_x, y_: test_y})))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.6666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COqQC-9bTgbE",
        "colab_type": "code",
        "outputId": "477f0e03-bfb5-45f9-8787-eeabd7c58eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Print the final mean square error\n",
        " \n",
        "pred_y = sess.run(y, feed_dict={x: test_x})\n",
        "mse = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "print(\"MSE: %.4f\" % sess.run(mse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: 3.1407\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}